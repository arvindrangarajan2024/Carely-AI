{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DK4gXsuQKfh2"
      },
      "source": [
        "# Multilingual AI Agent Demo\n",
        "\n",
        "A specialized AI agent that detects and processes text in **50+ languages** using a hybrid approach combining statistical methods and OpenAI LLM.\n",
        "\n",
        "## Features\n",
        "\n",
        "- **50+ Languages**: English, Spanish, French, German, Chinese, Japanese, Arabic, Russian, and more\n",
        "- **Hybrid Detection**: Combines `langdetect` (fast) with OpenAI (context-aware)\n",
        "- **Confidence Scoring**: Provides reliability metrics for each detection\n",
        "- **Batch Processing**: Efficiently process multiple texts\n",
        "- **Translation Suggestions**: Get translations for detected text\n",
        "- **Statistics Tracking**: Monitor detection history and language distribution\n",
        "\n",
        "## Detection Methods\n",
        "\n",
        "1. **Statistical Detection** (langdetect) - Fast, character n-gram based\n",
        "2. **AI-Powered Detection** (OpenAI) - Context-aware, handles nuances\n",
        "3. **Hybrid Mode** - Combines both for maximum accuracy\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OGcTuaDdKfh3"
      },
      "source": [
        "## 1. Setup and Imports\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4DkmhZWl0WeG",
        "outputId": "8056774a-f80b-4917-b3ff-bd1f213af8c1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cv4gq1iOKfh3",
        "outputId": "508aeb04-8962-4b20-e08a-28592911b21e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All packages imported successfully\n",
            "OpenAI API key found - \n"
          ]
        }
      ],
      "source": [
        "# Install required packages (uncomment if needed)\n",
        "!pip install openai langdetect -q\n",
        "\n",
        "import os\n",
        "import json\n",
        "from typing import Dict, List, Any, Optional\n",
        "from datetime import datetime\n",
        "\n",
        "# Import required libraries\n",
        "try:\n",
        "    from openai import OpenAI\n",
        "    from langdetect import detect, detect_langs, LangDetectException\n",
        "    print(\"All packages imported successfully\")\n",
        "except ImportError as e:\n",
        "    print(f\"Error: {e}\")\n",
        "    print(\"\\nPlease install required packages:\")\n",
        "    print(\"  pip install openai langdetect\")\n",
        "\n",
        "# Initialize OpenAI client\n",
        "# Make sure to set your API key: export OPENAI_API_KEY='your-key-here'\n",
        "# client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\")) # Original line\n",
        "from google.colab import userdata\n",
        "client = OpenAI(api_key=userdata.get('OPENAI_API_KEY'))\n",
        "\n",
        "\n",
        "if userdata.get(\"OPENAI_API_KEY\"):\n",
        "    print(\"OpenAI API key found - \")\n",
        "else:\n",
        "    print(\" Warning: OPENAI_API_KEY not set. Please set it before running AI detection.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y3lzZ2GyKfh4"
      },
      "source": [
        "## 2. MultilingualAgent Class\n",
        "\n",
        "The core class that handles language detection using multiple methods.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-BIQhtGNKfh4",
        "outputId": "48785623-5620-42aa-8150-285e103c8753"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ MultilingualAgent class defined\n"
          ]
        }
      ],
      "source": [
        "class MultilingualAgent:\n",
        "    \"\"\"\n",
        "    AI Agent specialized in detecting and processing multiple languages.\n",
        "    \"\"\"\n",
        "\n",
        "    # Comprehensive language mapping (50+ languages)\n",
        "    LANGUAGE_NAMES = {\n",
        "        'af': 'Afrikaans', 'ar': 'Arabic', 'bg': 'Bulgarian', 'bn': 'Bengali',\n",
        "        'ca': 'Catalan', 'cs': 'Czech', 'cy': 'Welsh', 'da': 'Danish',\n",
        "        'de': 'German', 'el': 'Greek', 'en': 'English', 'es': 'Spanish',\n",
        "        'et': 'Estonian', 'fa': 'Persian', 'fi': 'Finnish', 'fr': 'French',\n",
        "        'gu': 'Gujarati', 'he': 'Hebrew', 'hi': 'Hindi', 'hr': 'Croatian',\n",
        "        'hu': 'Hungarian', 'id': 'Indonesian', 'it': 'Italian', 'ja': 'Japanese',\n",
        "        'kn': 'Kannada', 'ko': 'Korean', 'lt': 'Lithuanian', 'lv': 'Latvian',\n",
        "        'mk': 'Macedonian', 'ml': 'Malayalam', 'mr': 'Marathi', 'ne': 'Nepali',\n",
        "        'nl': 'Dutch', 'no': 'Norwegian', 'pa': 'Punjabi', 'pl': 'Polish',\n",
        "        'pt': 'Portuguese', 'ro': 'Romanian', 'ru': 'Russian', 'sk': 'Slovak',\n",
        "        'sl': 'Slovenian', 'so': 'Somali', 'sq': 'Albanian', 'sv': 'Swedish',\n",
        "        'sw': 'Swahili', 'ta': 'Tamil', 'te': 'Telugu', 'th': 'Thai',\n",
        "        'tl': 'Tagalog', 'tr': 'Turkish', 'uk': 'Ukrainian', 'ur': 'Urdu',\n",
        "        'vi': 'Vietnamese', 'zh-cn': 'Chinese (Simplified)', 'zh-tw': 'Chinese (Traditional)'\n",
        "    }\n",
        "\n",
        "    def __init__(self, model: str = \"gpt-4o-mini\"):\n",
        "        \"\"\"\n",
        "        Initialize the multilingual agent.\n",
        "\n",
        "        Args:\n",
        "            model: OpenAI model to use (default: gpt-4o-mini)\n",
        "        \"\"\"\n",
        "        from google.colab import userdata\n",
        "        self.client = OpenAI(api_key=userdata.get(\"OPENAI_API_KEY\"))\n",
        "        self.model = model\n",
        "        self.detection_history: List[Dict] = []\n",
        "\n",
        "        print(\" Multilingual Agent initialized\")\n",
        "        print(f\"   Model: {self.model}\")\n",
        "        print(f\"   Languages supported: {len(self.LANGUAGE_NAMES)}+\")\n",
        "\n",
        "    def detect_language_basic(self, text: str) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Basic language detection using langdetect library.\n",
        "        Fast and reliable for single-language text.\n",
        "\n",
        "        Args:\n",
        "            text: Text to analyze\n",
        "\n",
        "        Returns:\n",
        "            Dictionary with detection results\n",
        "        \"\"\"\n",
        "        try:\n",
        "            lang_probs = detect_langs(text)\n",
        "            primary = lang_probs[0]\n",
        "\n",
        "            result = {\n",
        "                \"method\": \"langdetect\",\n",
        "                \"primary_language\": {\n",
        "                    \"code\": primary.lang,\n",
        "                    \"name\": self.LANGUAGE_NAMES.get(primary.lang, primary.lang.upper()),\n",
        "                    \"confidence\": round(primary.prob, 3)\n",
        "                },\n",
        "                \"all_detected\": [\n",
        "                    {\n",
        "                        \"code\": lp.lang,\n",
        "                        \"name\": self.LANGUAGE_NAMES.get(lp.lang, lp.lang.upper()),\n",
        "                        \"confidence\": round(lp.prob, 3)\n",
        "                    }\n",
        "                    for lp in lang_probs[:3]  # Top 3 candidates\n",
        "                ],\n",
        "                \"text_length\": len(text),\n",
        "                \"timestamp\": datetime.now().isoformat()\n",
        "            }\n",
        "\n",
        "            return result\n",
        "\n",
        "        except LangDetectException as e:\n",
        "            return {\n",
        "                \"method\": \"langdetect\",\n",
        "                \"error\": str(e),\n",
        "                \"primary_language\": {\"code\": \"unknown\", \"name\": \"Unknown\"}\n",
        "            }\n",
        "\n",
        "    def detect_language_ai(self, text: str) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        AI-powered language detection using OpenAI.\n",
        "        More context-aware and can handle mixed languages.\n",
        "\n",
        "        Args:\n",
        "            text: Text to analyze\n",
        "\n",
        "        Returns:\n",
        "            Dictionary with AI detection results\n",
        "        \"\"\"\n",
        "        prompt = f\"\"\"Analyze the following text and identify its language(s).\n",
        "\n",
        "Text: \"{text}\"\n",
        "\n",
        "Provide:\n",
        "1. Primary language (code and name)\n",
        "2. Confidence level (0-1)\n",
        "3. Any secondary languages detected\n",
        "4. Whether it's formal or informal\n",
        "5. Any notable linguistic features\n",
        "\n",
        "Respond in JSON format.\"\"\"\n",
        "\n",
        "        try:\n",
        "            response = self.client.chat.completions.create(\n",
        "                model=self.model,\n",
        "                messages=[\n",
        "                    {\"role\": \"system\", \"content\": \"You are a linguistic expert specialized in language identification. Respond only with valid JSON.\"},\n",
        "                    {\"role\": \"user\", \"content\": prompt}\n",
        "                ],\n",
        "                temperature=0.3\n",
        "            )\n",
        "\n",
        "            result = json.loads(response.choices[0].message.content)\n",
        "            result[\"method\"] = \"openai_llm\"\n",
        "            result[\"timestamp\"] = datetime.now().isoformat()\n",
        "\n",
        "            return result\n",
        "\n",
        "        except Exception as e:\n",
        "            return {\n",
        "                \"method\": \"openai_llm\",\n",
        "                \"error\": str(e),\n",
        "                \"primary_language\": {\"code\": \"unknown\", \"name\": \"Unknown\"}\n",
        "            }\n",
        "\n",
        "    def detect_language_hybrid(self, text: str, verbose: bool = True) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Hybrid detection: combines langdetect and OpenAI for best accuracy.\n",
        "\n",
        "        Args:\n",
        "            text: Text to analyze\n",
        "            verbose: Print detection process\n",
        "\n",
        "        Returns:\n",
        "            Combined detection results\n",
        "        \"\"\"\n",
        "        if verbose:\n",
        "            print(f\"\\n{'='*70}\")\n",
        "            print(\"LANGUAGE DETECTION - HYBRID MODE\")\n",
        "            print(f\"{'='*70}\")\n",
        "            print(f\"\\nText: \\\"{text[:100]}{'...' if len(text) > 100 else ''}\\\"\")\n",
        "            print(f\"Length: {len(text)} characters\\n\")\n",
        "\n",
        "        # Method 1: Fast detection with langdetect\n",
        "        if verbose:\n",
        "            print(\"Method 1: Statistical Detection (langdetect)\")\n",
        "        basic_result = self.detect_language_basic(text)\n",
        "\n",
        "        if verbose and \"error\" not in basic_result:\n",
        "            print(f\"   Result: {basic_result['primary_language']['name']} \"\n",
        "                  f\"({basic_result['primary_language']['confidence']} confidence)\")\n",
        "\n",
        "        # Method 2: AI-powered detection\n",
        "        if verbose:\n",
        "            print(\"\\nMethod 2: AI-Powered Detection (OpenAI)\")\n",
        "        ai_result = self.detect_language_ai(text)\n",
        "\n",
        "        if verbose and \"error\" not in ai_result:\n",
        "            if isinstance(ai_result.get('primary_language'), dict):\n",
        "                lang_name = ai_result['primary_language'].get('name', 'Unknown')\n",
        "                ai_confidence = ai_result['primary_language'].get('confidence', 'N/A') # Safely access confidence\n",
        "            else:\n",
        "                lang_name = ai_result.get('primary_language', 'Unknown')\n",
        "                ai_confidence = 'N/A' # Set confidence to N/A if result is not a dict\n",
        "            print(f\"   Result: {lang_name}\")\n",
        "            if ai_confidence != 'N/A':\n",
        "                print(f\"   Confidence: {ai_confidence}\")\n",
        "\n",
        "\n",
        "        # Combine results\n",
        "        combined = {\n",
        "            \"text\": text[:200] + \"...\" if len(text) > 200 else text,\n",
        "            \"statistical_detection\": basic_result,\n",
        "            \"ai_detection\": ai_result,\n",
        "            \"timestamp\": datetime.now().isoformat(),\n",
        "            \"text_length\": len(text)\n",
        "        }\n",
        "\n",
        "        # Determine final verdict\n",
        "        if \"error\" not in basic_result and \"error\" not in ai_result:\n",
        "            basic_lang = basic_result['primary_language']['code']\n",
        "            ai_lang = ai_result.get('primary_language', {})\n",
        "            ai_lang_code = ai_lang.get('code', '').lower() if isinstance(ai_lang, dict) else ''\n",
        "\n",
        "            if basic_lang == ai_lang_code:\n",
        "                combined[\"verdict\"] = \"Both methods agree\"\n",
        "                combined[\"confidence\"] = \"high\"\n",
        "            else:\n",
        "                combined[\"verdict\"] = \"Methods differ - review recommended\"\n",
        "                combined[\"confidence\"] = \"medium\"\n",
        "        else:\n",
        "            combined[\"verdict\"] = \"Detection issues encountered\"\n",
        "            combined[\"confidence\"] = \"low\"\n",
        "\n",
        "        # Store in history\n",
        "        self.detection_history.append(combined)\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"\\n{combined['verdict']}\")\n",
        "            print(f\"{'='*70}\\n\")\n",
        "\n",
        "        return combined\n",
        "\n",
        "    def detect_batch(self, texts: List[str]) -> List[Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Detect languages for multiple texts.\n",
        "\n",
        "        Args:\n",
        "            texts: List of texts to analyze\n",
        "\n",
        "        Returns:\n",
        "            List of detection results\n",
        "        \"\"\"\n",
        "        print(f\"\\nProcessing {len(texts)} texts...\")\n",
        "        results = []\n",
        "\n",
        "        for i, text in enumerate(texts, 1):\n",
        "            print(f\"\\n[{i}/{len(texts)}] \", end=\"\")\n",
        "            result = self.detect_language_hybrid(text, verbose=False)\n",
        "            results.append(result)\n",
        "\n",
        "            # Print summary\n",
        "            basic = result['statistical_detection']\n",
        "            if 'error' not in basic:\n",
        "                lang = basic['primary_language']\n",
        "                print(f\"✓ {lang['name']} ({lang['confidence']})\")\n",
        "            else:\n",
        "                print(f\"✗ Detection failed\")\n",
        "\n",
        "        return results\n",
        "\n",
        "    def get_statistics(self) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Get statistics about detected languages.\n",
        "\n",
        "        Returns:\n",
        "            Statistics dictionary\n",
        "        \"\"\"\n",
        "        if not self.detection_history:\n",
        "            return {\"message\": \"No detections performed yet\"}\n",
        "\n",
        "        lang_counts = {}\n",
        "        total_confidence = 0\n",
        "        successful_detections = 0\n",
        "\n",
        "        for detection in self.detection_history:\n",
        "            basic = detection.get('statistical_detection', {})\n",
        "            if 'error' not in basic:\n",
        "                lang = basic['primary_language']['name']\n",
        "                lang_counts[lang] = lang_counts.get(lang, 0) + 1\n",
        "                total_confidence += basic['primary_language']['confidence']\n",
        "                successful_detections += 1\n",
        "\n",
        "        return {\n",
        "            \"total_detections\": len(self.detection_history),\n",
        "            \"successful_detections\": successful_detections,\n",
        "            \"languages_detected\": len(lang_counts),\n",
        "            \"language_breakdown\": lang_counts,\n",
        "            \"average_confidence\": round(total_confidence / successful_detections, 3) if successful_detections > 0 else 0\n",
        "        }\n",
        "\n",
        "    def translate_suggestion(self, text: str, target_language: str = \"English\") -> str:\n",
        "        \"\"\"\n",
        "        Suggest translation for detected text.\n",
        "\n",
        "        Args:\n",
        "            text: Text to potentially translate\n",
        "            target_language: Target language for translation\n",
        "\n",
        "        Returns:\n",
        "            Translation suggestion\n",
        "        \"\"\"\n",
        "        # First detect the language\n",
        "        detection = self.detect_language_basic(text)\n",
        "        source_lang = detection['primary_language']['name']\n",
        "\n",
        "        if source_lang.lower() == target_language.lower():\n",
        "            return f\"Text is already in {target_language}\"\n",
        "\n",
        "        prompt = f\"\"\"Translate the following text from {source_lang} to {target_language}:\n",
        "\n",
        "\"{text}\"\n",
        "\n",
        "Provide only the translation, nothing else.\"\"\"\n",
        "\n",
        "        try:\n",
        "            response = self.client.chat.completions.create(\n",
        "                model=self.model,\n",
        "                messages=[\n",
        "                    {\"role\": \"system\", \"content\": \"You are a professional translator.\"},\n",
        "                    {\"role\": \"user\", \"content\": prompt}\n",
        "                ],\n",
        "                temperature=0.3\n",
        "            )\n",
        "\n",
        "            return response.choices[0].message.content\n",
        "\n",
        "        except Exception as e:\n",
        "            return f\"Translation error: {e}\"\n",
        "\n",
        "print(\"✓ MultilingualAgent class defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ooVaVwJvKfh4"
      },
      "source": [
        "## 3. Initialize Agent\n",
        "\n",
        "Create an instance of the MultilingualAgent.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eG9DKhY8Kfh4",
        "outputId": "0d8f88ef-1feb-47fd-e8c0-9506694f8dba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Multilingual Agent initialized\n",
            "   Model: gpt-4o-mini\n",
            "   Languages supported: 55+\n"
          ]
        }
      ],
      "source": [
        "# Initialize the agent\n",
        "agent = MultilingualAgent(model=\"gpt-4o-mini\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NqKVpD6iKfh5"
      },
      "source": [
        "## 4. Demo: Individual Language Detection\n",
        "\n",
        "Test the agent with various languages one by one.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h0MSBq4eKfh5",
        "outputId": "26dffe09-f3ee-4bc0-8af7-f1b96e6410fd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "TEST 1: INDIVIDUAL LANGUAGE DETECTION\n",
            "======================================================================\n",
            "\n",
            "[Test 1] Expected: English\n",
            "\n",
            "======================================================================\n",
            "LANGUAGE DETECTION - HYBRID MODE\n",
            "======================================================================\n",
            "\n",
            "Text: \"Hello, how are you today?\"\n",
            "Length: 25 characters\n",
            "\n",
            "Method 1: Statistical Detection (langdetect)\n",
            "   Result: Somali (0.571 confidence)\n",
            "\n",
            "Method 2: AI-Powered Detection (OpenAI)\n",
            "\n",
            "Detection issues encountered\n",
            "======================================================================\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(\"=\"*70)\n",
        "print(\"TEST 1: INDIVIDUAL LANGUAGE DETECTION\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Test English\n",
        "print(\"\\n[Test 1] Expected: English\")\n",
        "result = agent.detect_language_hybrid(\"Hello, how are you today?\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qWWBLe4ZKfh5",
        "outputId": "446152ce-d0eb-4f82-a3dd-93a67eef5e8b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[Test 2] Expected: French\n",
            "\n",
            "======================================================================\n",
            "LANGUAGE DETECTION - HYBRID MODE\n",
            "======================================================================\n",
            "\n",
            "Text: \"Bonjour, comment allez-vous aujourd'hui?\"\n",
            "Length: 40 characters\n",
            "\n",
            "Method 1: Statistical Detection (langdetect)\n",
            "   Result: French (1.0 confidence)\n",
            "\n",
            "Method 2: AI-Powered Detection (OpenAI)\n",
            "\n",
            "Detection issues encountered\n",
            "======================================================================\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Test French\n",
        "print(\"\\n[Test 2] Expected: French\")\n",
        "result = agent.detect_language_hybrid(\"Bonjour, comment allez-vous aujourd'hui?\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oYhbSYMdKfh5",
        "outputId": "993e4180-6389-49ad-d79c-bc00fd92af12"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[Test 3] Expected: Spanish\n",
            "\n",
            "======================================================================\n",
            "LANGUAGE DETECTION - HYBRID MODE\n",
            "======================================================================\n",
            "\n",
            "Text: \"Hola, ¿cómo estás hoy?\"\n",
            "Length: 22 characters\n",
            "\n",
            "Method 1: Statistical Detection (langdetect)\n",
            "   Result: Spanish (1.0 confidence)\n",
            "\n",
            "Method 2: AI-Powered Detection (OpenAI)\n",
            "\n",
            "Detection issues encountered\n",
            "======================================================================\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Test Spanish\n",
        "print(\"\\n[Test 3] Expected: Spanish\")\n",
        "result = agent.detect_language_hybrid(\"Hola, ¿cómo estás hoy?\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DAbBOnzuKfh5",
        "outputId": "b65d5ca6-b277-4482-811d-96232d20f783"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[Test 4] Expected: Japanese\n",
            "\n",
            "======================================================================\n",
            "LANGUAGE DETECTION - HYBRID MODE\n",
            "======================================================================\n",
            "\n",
            "Text: \"こんにちは、今日はお元気ですか？\"\n",
            "Length: 16 characters\n",
            "\n",
            "Method 1: Statistical Detection (langdetect)\n",
            "   Result: Japanese (1.0 confidence)\n",
            "\n",
            "Method 2: AI-Powered Detection (OpenAI)\n"
          ]
        }
      ],
      "source": [
        "# Test Japanese\n",
        "print(\"\\n[Test 4] Expected: Japanese\")\n",
        "result = agent.detect_language_hybrid(\"こんにちは、今日はお元気ですか？\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rhhxsAbyKfh5"
      },
      "outputs": [],
      "source": [
        "# Test German\n",
        "print(\"\\n[Test 5] Expected: German\")\n",
        "result = agent.detect_language_hybrid(\"Guten Tag, wie geht es Ihnen heute?\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IkXmJPuFKfh5"
      },
      "source": [
        "## 5. Demo: Batch Processing\n",
        "\n",
        "Process multiple texts efficiently.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZVDNcWOqKfh5"
      },
      "outputs": [],
      "source": [
        "print(\"=\"*70)\n",
        "print(\"TEST 2: BATCH PROCESSING\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "batch_texts = [\n",
        "    \"Привет, как дела сегодня?\",  # Russian\n",
        "    \"你好，你今天好吗？\",  # Chinese\n",
        "    \"مرحبا، كيف حالك اليوم؟\",  # Arabic\n",
        "    \"Ciao, come stai oggi?\",  # Italian\n",
        "    \"Olá, como você está hoje?\"  # Portuguese\n",
        "]\n",
        "\n",
        "batch_results = agent.detect_batch(batch_texts)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gq2-xAYNKfh5"
      },
      "source": [
        "## 6. Demo: Translation\n",
        "\n",
        "Detect language and translate to English.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "26pBpuq3Kfh5"
      },
      "outputs": [],
      "source": [
        "print(\"=\"*70)\n",
        "print(\"TEST 3: TRANSLATION\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "spanish_text = \"Hola, necesito ayuda con mi computadora\"\n",
        "print(f\"\\nOriginal (Spanish): {spanish_text}\")\n",
        "\n",
        "translation = agent.translate_suggestion(spanish_text, \"English\")\n",
        "print(f\"Translation (English): {translation}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wX1q0AKyKfh5"
      },
      "outputs": [],
      "source": [
        "# Try another language\n",
        "french_text = \"Je voudrais réserver une table pour deux personnes\"\n",
        "print(f\"\\nOriginal (French): {french_text}\")\n",
        "\n",
        "translation = agent.translate_suggestion(french_text, \"English\")\n",
        "print(f\"Translation (English): {translation}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ya2fsClOKfh5"
      },
      "source": [
        "## 7. Statistics\n",
        "\n",
        "View detection history and language distribution.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BCn7tOzuKfh5"
      },
      "outputs": [],
      "source": [
        "print(\"=\"*70)\n",
        "print(\"TEST 4: STATISTICS\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "stats = agent.get_statistics()\n",
        "\n",
        "print(\"\\n Detection Statistics:\")\n",
        "print(f\"   Total detections: {stats['total_detections']}\")\n",
        "print(f\"   Successful: {stats['successful_detections']}\")\n",
        "print(f\"   Unique languages: {stats['languages_detected']}\")\n",
        "print(f\"   Average confidence: {stats['average_confidence']}\")\n",
        "\n",
        "print(\"\\n   Language breakdown:\")\n",
        "for lang, count in sorted(stats['language_breakdown'].items(), key=lambda x: x[1], reverse=True):\n",
        "    print(f\"      • {lang}: {count}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"DEMONSTRATION COMPLETE\")\n",
        "print(\"=\"*70)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l_Cn4UyLKfh6"
      },
      "source": [
        "## 8. Try Your Own Text\n",
        "\n",
        "Test with your own text samples below.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q81WjeIrKfh6"
      },
      "outputs": [],
      "source": [
        "# Test with your own text\n",
        "your_text = \"yêu hòa bình và niềm vui\"\n",
        "\n",
        "# Uncomment to test:\n",
        "result = agent.detect_language_hybrid(your_text)\n",
        "\n",
        "print(result)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "INbjoepPKfh6"
      },
      "source": [
        "## 9. More Examples\n",
        "\n",
        "Additional language samples to test.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tD4KfeeiKfh6"
      },
      "outputs": [],
      "source": [
        "# Test more languages\n",
        "more_languages = {\n",
        "    \"Hindi\": \"नमस्ते, आप कैसे हैं?\",\n",
        "    \"Korean\": \"안녕하세요, 어떻게 지내세요?\",\n",
        "    \"Turkish\": \"Merhaba, nasılsın?\",\n",
        "    \"Thai\": \"สวัสดี คุณเป็นอย่างไรบ้าง?\",\n",
        "    \"Vietnamese\": \"Xin chào, bạn khỏe không?\",\n",
        "    \"Dutch\": \"Hallo, hoe gaat het met je?\",\n",
        "    \"Swedish\": \"Hej, hur mår du?\",\n",
        "    \"Polish\": \"Cześć, jak się masz?\"\n",
        "}\n",
        "\n",
        "print(\"\\nTesting Additional Languages:\\n\")\n",
        "for expected_lang, text in more_languages.items():\n",
        "    print(f\"Expected: {expected_lang}\")\n",
        "    print(f\"Text: {text}\")\n",
        "    result = agent.detect_language_basic(text)\n",
        "    detected = result['primary_language']\n",
        "    print(f\"✓ Detected: {detected['name']} (confidence: {detected['confidence']})\")\n",
        "    print(\"-\" * 70)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2qOmyFpCKfh6"
      },
      "source": [
        "## 10. View Detection History\n",
        "\n",
        "Examine all detections performed in this session.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qZ9NM21AKfh6"
      },
      "outputs": [],
      "source": [
        "# View detection history\n",
        "print(f\"Detection History ({len(agent.detection_history)} total)\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "for i, detection in enumerate(agent.detection_history[-5:], 1):  # Show last 5\n",
        "    print(f\"\\n[{i}] Text: {detection['text'][:60]}...\")\n",
        "    stat = detection['statistical_detection']\n",
        "    if 'error' not in stat:\n",
        "        lang = stat['primary_language']\n",
        "        print(f\"    Language: {lang['name']} ({lang['confidence']} confidence)\")\n",
        "        print(f\"    Verdict: {detection['verdict']}\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}